{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Spark Libraries, Visualization Libraries, Pandas-Numpy (Data Manipualtion) Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark Context Session\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/23 15:11:46 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.172 instead (on interface en0)\n",
      "22/10/23 15:11:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/23 15:11:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Spark Connection\n",
    "spark = SparkContext(\"local\", \"PySpark Word Count Generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>----------------------- Readme.md File -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = '/Users/HarshitGaur/Documents/spark/README.md'\n",
    "pysparkDf = spark.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To cache the data of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/HarshitGaur/Documents/spark/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysparkDf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check the count of Lines in the Readme file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysparkDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To split the data into Lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineData = pysparkDf.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To count the words in the Readme file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcData = lineData.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Show the generated Word Count List of the Readme file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#', 1)\n",
      "('Apache', 1)\n",
      "('Spark', 15)\n",
      "('', 41)\n",
      "('is', 7)\n",
      "('a', 9)\n",
      "('unified', 1)\n",
      "('analytics', 1)\n",
      "('engine', 2)\n",
      "('for', 13)\n",
      "('large-scale', 1)\n",
      "('data', 2)\n",
      "('processing.', 2)\n",
      "('It', 2)\n",
      "('provides', 1)\n",
      "('high-level', 1)\n",
      "('APIs', 1)\n",
      "('in', 5)\n",
      "('Scala,', 1)\n",
      "('Java,', 1)\n",
      "('Python,', 2)\n",
      "('and', 9)\n",
      "('R,', 1)\n",
      "('an', 4)\n",
      "('optimized', 1)\n",
      "('that', 2)\n",
      "('supports', 2)\n",
      "('general', 2)\n",
      "('computation', 1)\n",
      "('graphs', 1)\n",
      "('analysis.', 1)\n",
      "('also', 5)\n",
      "('rich', 1)\n",
      "('set', 2)\n",
      "('of', 5)\n",
      "('higher-level', 1)\n",
      "('tools', 1)\n",
      "('including', 4)\n",
      "('SQL', 2)\n",
      "('DataFrames,', 1)\n",
      "('pandas', 2)\n",
      "('API', 1)\n",
      "('on', 8)\n",
      "('workloads,', 1)\n",
      "('MLlib', 1)\n",
      "('machine', 1)\n",
      "('learning,', 1)\n",
      "('GraphX', 1)\n",
      "('graph', 1)\n",
      "('processing,', 1)\n",
      "('Structured', 1)\n",
      "('Streaming', 1)\n",
      "('stream', 1)\n",
      "('<https://spark.apache.org/>', 1)\n",
      "('[![GitHub', 1)\n",
      "('Action', 1)\n",
      "('Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&event=push)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush)', 1)\n",
      "('[![AppVeyor', 1)\n",
      "('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', 1)\n",
      "('[![PySpark', 1)\n",
      "('Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)', 1)\n",
      "('##', 9)\n",
      "('Online', 1)\n",
      "('Documentation', 1)\n",
      "('You', 3)\n",
      "('can', 6)\n",
      "('find', 1)\n",
      "('the', 23)\n",
      "('latest', 1)\n",
      "('documentation,', 1)\n",
      "('programming', 1)\n",
      "('guide,', 1)\n",
      "('[project', 1)\n",
      "('web', 1)\n",
      "('page](https://spark.apache.org/documentation.html).', 1)\n",
      "('This', 2)\n",
      "('README', 1)\n",
      "('file', 1)\n",
      "('only', 1)\n",
      "('contains', 1)\n",
      "('basic', 1)\n",
      "('setup', 1)\n",
      "('instructions.', 1)\n",
      "('Building', 1)\n",
      "('built', 1)\n",
      "('using', 3)\n",
      "('[Apache', 1)\n",
      "('Maven](https://maven.apache.org/).', 1)\n",
      "('To', 2)\n",
      "('build', 3)\n",
      "('its', 1)\n",
      "('example', 3)\n",
      "('programs,', 1)\n",
      "('run:', 1)\n",
      "('```bash', 6)\n",
      "('./build/mvn', 1)\n",
      "('-DskipTests', 1)\n",
      "('clean', 1)\n",
      "('package', 1)\n",
      "('```', 8)\n",
      "('(You', 1)\n",
      "('do', 2)\n",
      "('not', 1)\n",
      "('need', 1)\n",
      "('to', 16)\n",
      "('this', 1)\n",
      "('if', 4)\n",
      "('you', 4)\n",
      "('downloaded', 1)\n",
      "('pre-built', 1)\n",
      "('package.)', 1)\n",
      "('More', 1)\n",
      "('detailed', 2)\n",
      "('documentation', 3)\n",
      "('available', 1)\n",
      "('from', 1)\n",
      "('project', 1)\n",
      "('site,', 1)\n",
      "('at', 2)\n",
      "('[\"Building', 1)\n",
      "('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1)\n",
      "('For', 3)\n",
      "('development', 1)\n",
      "('tips,', 1)\n",
      "('info', 1)\n",
      "('developing', 1)\n",
      "('IDE,', 1)\n",
      "('see', 3)\n",
      "('[\"Useful', 1)\n",
      "('Developer', 1)\n",
      "('Tools\"](https://spark.apache.org/developer-tools.html).', 1)\n",
      "('Interactive', 2)\n",
      "('Scala', 2)\n",
      "('Shell', 2)\n",
      "('The', 1)\n",
      "('easiest', 1)\n",
      "('way', 1)\n",
      "('start', 1)\n",
      "('through', 1)\n",
      "('shell:', 2)\n",
      "('./bin/spark-shell', 1)\n",
      "('Try', 1)\n",
      "('following', 2)\n",
      "('command,', 2)\n",
      "('which', 2)\n",
      "('should', 2)\n",
      "('return', 2)\n",
      "('1,000,000,000:', 2)\n",
      "('```scala', 1)\n",
      "('scala>', 1)\n",
      "('spark.range(1000', 2)\n",
      "('*', 4)\n",
      "('1000', 2)\n",
      "('1000).count()', 2)\n",
      "('Python', 2)\n",
      "('Alternatively,', 1)\n",
      "('prefer', 1)\n",
      "('use', 3)\n",
      "('./bin/pyspark', 1)\n",
      "('And', 1)\n",
      "('run', 7)\n",
      "('```python', 1)\n",
      "('>>>', 1)\n",
      "('Example', 1)\n",
      "('Programs', 1)\n",
      "('comes', 1)\n",
      "('with', 3)\n",
      "('several', 1)\n",
      "('sample', 1)\n",
      "('programs', 2)\n",
      "('`examples`', 2)\n",
      "('directory.', 1)\n",
      "('one', 2)\n",
      "('them,', 1)\n",
      "('`./bin/run-example', 1)\n",
      "('<class>', 1)\n",
      "('[params]`.', 1)\n",
      "('example:', 1)\n",
      "('./bin/run-example', 2)\n",
      "('SparkPi', 2)\n",
      "('will', 1)\n",
      "('Pi', 1)\n",
      "('locally.', 1)\n",
      "('MASTER', 1)\n",
      "('environment', 1)\n",
      "('variable', 1)\n",
      "('when', 1)\n",
      "('running', 1)\n",
      "('examples', 2)\n",
      "('submit', 1)\n",
      "('cluster.', 1)\n",
      "('be', 2)\n",
      "('mesos://', 1)\n",
      "('or', 3)\n",
      "('spark://', 1)\n",
      "('URL,', 1)\n",
      "('\"yarn\"', 1)\n",
      "('YARN,', 1)\n",
      "('\"local\"', 1)\n",
      "('locally', 2)\n",
      "('thread,', 1)\n",
      "('\"local[N]\"', 1)\n",
      "('N', 1)\n",
      "('threads.', 1)\n",
      "('abbreviated', 1)\n",
      "('class', 2)\n",
      "('name', 1)\n",
      "('package.', 1)\n",
      "('instance:', 1)\n",
      "('MASTER=spark://host:7077', 1)\n",
      "('Many', 1)\n",
      "('print', 1)\n",
      "('usage', 1)\n",
      "('help', 1)\n",
      "('no', 1)\n",
      "('params', 1)\n",
      "('are', 1)\n",
      "('given.', 1)\n",
      "('Running', 1)\n",
      "('Tests', 1)\n",
      "('Testing', 1)\n",
      "('first', 1)\n",
      "('requires', 1)\n",
      "('[building', 1)\n",
      "('Spark](#building-spark).', 1)\n",
      "('Once', 1)\n",
      "('built,', 1)\n",
      "('tests', 2)\n",
      "('using:', 1)\n",
      "('./dev/run-tests', 1)\n",
      "('Please', 4)\n",
      "('guidance', 2)\n",
      "('how', 3)\n",
      "('[run', 1)\n",
      "('module,', 1)\n",
      "('individual', 1)\n",
      "('tests](https://spark.apache.org/developer-tools.html#individual-tests).', 1)\n",
      "('There', 1)\n",
      "('Kubernetes', 1)\n",
      "('integration', 1)\n",
      "('test,', 1)\n",
      "('resource-managers/kubernetes/integration-tests/README.md', 1)\n",
      "('A', 1)\n",
      "('Note', 1)\n",
      "('About', 1)\n",
      "('Hadoop', 3)\n",
      "('Versions', 1)\n",
      "('uses', 1)\n",
      "('core', 1)\n",
      "('library', 1)\n",
      "('talk', 1)\n",
      "('HDFS', 1)\n",
      "('other', 1)\n",
      "('Hadoop-supported', 1)\n",
      "('storage', 1)\n",
      "('systems.', 1)\n",
      "('Because', 1)\n",
      "('protocols', 1)\n",
      "('have', 1)\n",
      "('changed', 1)\n",
      "('different', 1)\n",
      "('versions', 1)\n",
      "('Hadoop,', 2)\n",
      "('must', 1)\n",
      "('against', 1)\n",
      "('same', 1)\n",
      "('version', 1)\n",
      "('your', 1)\n",
      "('cluster', 1)\n",
      "('runs.', 1)\n",
      "('refer', 2)\n",
      "('[\"Specifying', 1)\n",
      "('Version', 1)\n",
      "('Enabling', 1)\n",
      "('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 1)\n",
      "('building', 2)\n",
      "('particular', 2)\n",
      "('distribution', 1)\n",
      "('Hive', 2)\n",
      "('Thriftserver', 1)\n",
      "('distributions.', 1)\n",
      "('Configuration', 1)\n",
      "('[Configuration', 1)\n",
      "('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1)\n",
      "('online', 1)\n",
      "('overview', 1)\n",
      "('configure', 1)\n",
      "('Spark.', 1)\n",
      "('Contributing', 1)\n",
      "('review', 1)\n",
      "('[Contribution', 1)\n",
      "('guide](https://spark.apache.org/contributing.html)', 1)\n",
      "('information', 1)\n",
      "('get', 1)\n",
      "('started', 1)\n",
      "('contributing', 1)\n",
      "('project.', 1)\n"
     ]
    }
   ],
   "source": [
    "for word in wcData.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Hadoop Count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of 'Hadoop' word is:  3\n"
     ]
    }
   ],
   "source": [
    "for word in wcData.collect():\n",
    "    if (word[0] == \"Hadoop\"):\n",
    "        print(\"The count of 'Hadoop' word is: \", word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Most Common Word? How many times did it occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common word:  the\n",
      "Occurrences:  23\n"
     ]
    }
   ],
   "source": [
    "maxCount = 0\n",
    "maxCountWord = ''\n",
    "\n",
    "for word in wcData.collect():\n",
    "    if (word[1] > maxCount and word[0] != ''):\n",
    "        maxCountWord = word[0]\n",
    "        maxCount = word[1]\n",
    "\n",
    "print(\"The most common word: \", maxCountWord)\n",
    "print(\"Occurrences: \", maxCount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Least Common Word? How many times did it occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common word:  project.\n",
      "Occurrences:  1\n"
     ]
    }
   ],
   "source": [
    "minCount = 1\n",
    "minCountWord = ''\n",
    "\n",
    "for word in wcData.collect():\n",
    "    if (word[1] <= minCount and word[0] != ''):\n",
    "        minCountWord = word[0]\n",
    "        minCount = word[1]\n",
    "\n",
    "print(\"The most common word: \", minCountWord)\n",
    "print(\"Occurrences: \", minCount)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8594b24c37692db55ff3d81b921d1571d31b17b39641932a013aefdfbad5308"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
